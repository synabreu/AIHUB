{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_test.ipynb",
      "provenance": [],
      "private_outputs": true,
      "mount_file_id": "1d90T6gSEjQ_CWOTq0xP5dXQSeWUyhbeP",
      "authorship_tag": "ABX9TyN2UXiHPfzhSjyd5aue2rqd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/synabreu/AIHUB/blob/master/seq2seq_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12-WarADBLMg",
        "colab_type": "text"
      },
      "source": [
        "Sequence-to-Sequence learning at character-level using Keras\n",
        "\n",
        "This script demonstrates how to implement a basic character-level sequence-to-sequence model. I apply it to translating short English sentences into Korean sentences by character-by-character. \n",
        "Note that it is fairly unusual to do character-level machine translation, as word-level models are more common in this domain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVj94eo0q0Rv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt4DHtHPq_aK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYaRmnrmrGz7",
        "colab_type": "text"
      },
      "source": [
        "New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNTmW-CCseC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch Size of training dataset, 일반적으로 64, 129\n",
        "batch_size = 64 \n",
        "\n",
        "# Number of epochs to train for.\n",
        "epochs = 300 # 100 일때는 batch size 64, 200 일때 domain unmatched. 500 - 1000 일때는 128\n",
        "\n",
        "# Latent dimensionality of the encoding space. #256 is default.\n",
        "latent_dim = 1024 \n",
        "\n",
        "# Number of samples to train on\n",
        "num_samples = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtX6rEaet82V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Path to the data txt file on google drive. \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# data_path = 'drive/My Drive/Colab Notebooks/kor-utf8_2.txt'  \n",
        "data_path = 'drive/My Drive/Colab Notebooks/KOR_190920_UTF8.txt'  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OWIXoRUC5Oe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read sample data if data reads successful or not. \n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
        "\n",
        "lines= pd.read_table(data_path, names=['eng', 'kor'])\n",
        "lines = lines[0:10000]\n",
        "lines.shape\n",
        "lines.sample(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_Yq-vGCuEPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize variables, arrays and set for Vectorizing the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKusunJXuLH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(data_path, encoding='UTF8') as f:\n",
        "# with open(data_path, encoding='UTF16') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text = line.split('\\t')\n",
        "    \n",
        "    # we use \"tab(\\t)\" as the \"start sequence\" character for the targets, and \"\\n\" as \"end sequence\" character\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    \n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    \n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "            \n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEs-dMq1Jc-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = set(list(target_characters))\n",
        "\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skcBBVc5Jk1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Number of Samples:', len(input_texts))\n",
        "print('Number of Unique Input Tokens:', num_encoder_tokens)\n",
        "print('Number of Unique Output Tokens:', num_decoder_tokens)\n",
        "print('Max Sequence Length for Inputs:', max_encoder_seq_length)\n",
        "print('Max Sequence Length of Outputs:', max_decoder_seq_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2sAa56iJqDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9APsk86JuRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
        "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
        "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0Ty2FzRJxf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    \n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    encoder_input_data[i, t+1:, input_token_index[' ']] = 1.\n",
        "    \n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep.\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "            \n",
        "    decoder_input_data[i, t+1:, target_token_index[' ']] = 1.\n",
        "    decoder_target_data[i, t:, target_token_index[' ']] = 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87yy755gJ19s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "# discard 'encoder_outputs' and 'encoder_states' as initial state.\n",
        "encoder_states = [state_h, state_c]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpJUtwz2J6rk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up the decoder, using 'encoder_states' as inital state.\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F03iqrn_J9yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up our decoder to return full output sequences, and to return internal states as well.\n",
        "# Don't use the return states in the training model, but I will use them in inference.  \n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCdgiHe-KBPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the model that will turn 'encoder_input_data' into 'decoder_target_data'\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahOlEvBcKEf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model compile\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2EAuwVSKHhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "         batch_size=batch_size, epochs=epochs, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRjdFa3jmMHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate the model\n",
        "# scores = model.evaluate(encoder_inputs, encoder_states, verbose=0)\n",
        "# print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "\n",
        "# save the model\n",
        "save_path = 'drive/My Drive/Colab Notebooks/s2s_0505_01.h5'  \n",
        "model.save(save_path)\n",
        "\n",
        "# summerize the model\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS3Oqi7bI0Nc",
        "colab_type": "text"
      },
      "source": [
        "# Inference mode (sampling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Saye4Kz5I6Uh",
        "colab_type": "text"
      },
      "source": [
        "1) encode input and retrieve initial decoer state.\n",
        "\n",
        "2) run one step of decoder with this initial state and a \"start of sequence' token as target.\n",
        "\n",
        "3) Output will be the next target token.\n",
        "\n",
        "4) Repeat with the current target token and current states."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmINKsfAJGRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define sampling models\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim, ))\n",
        "decoder_state_input_c = Input(shape=(latent_dim, ))\n",
        "\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vfVF5R2ebPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reverse lookup token index to decode sequences back to something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i,char) for char, i in target_token_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y6AByRLe5yZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "\n",
        "  # Encode the input as state vectors.\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "  # Generate empty target sequence of length 1.\n",
        "  target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "  # Populate the first character of target sequence with the start character.\n",
        "  target_seq[0, 0, target_token_index['\\t']] = 1. \n",
        "\n",
        "  # Sampling loop for a batch of sequences to simplify it. and assume that the batch size of 1\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "\n",
        "  while not stop_condition:\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "    # Sample a token\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "    decoded_sentence += sampled_char \n",
        "\n",
        "    # Exit condition either hit max length or find <stop> chracter.\n",
        "    if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n",
        "      stop_condition = True\n",
        "\n",
        "    # Update the target sequence or length 1. \n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, sampled_token_index] = 1. \n",
        "\n",
        "    # Update states.\n",
        "    states_value = [h, c]\n",
        "\n",
        "  return decoded_sentence\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiG1RVmfesYc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for seq_index in range(100):\n",
        "  # Take one sequence (part of the training set) for trying out decoding.\n",
        "  input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "  print('-')\n",
        "  print('Input sentence:', input_texts[seq_index])\n",
        "  print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}